{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2eeed46-df0a-4684-a5f4-c422a4498469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_source</th>\n",
       "      <th>industry</th>\n",
       "      <th>number_of_courses_viewed</th>\n",
       "      <th>annual_income</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>location</th>\n",
       "      <th>interaction_count</th>\n",
       "      <th>lead_score</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>79450.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>south_america</td>\n",
       "      <td>4</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>social_media</td>\n",
       "      <td>retail</td>\n",
       "      <td>1</td>\n",
       "      <td>46992.0</td>\n",
       "      <td>employed</td>\n",
       "      <td>south_america</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>events</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>5</td>\n",
       "      <td>78796.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>australia</td>\n",
       "      <td>3</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>retail</td>\n",
       "      <td>2</td>\n",
       "      <td>83843.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>australia</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>referral</td>\n",
       "      <td>education</td>\n",
       "      <td>3</td>\n",
       "      <td>85012.0</td>\n",
       "      <td>self_employed</td>\n",
       "      <td>europe</td>\n",
       "      <td>3</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lead_source    industry  number_of_courses_viewed  annual_income  \\\n",
       "0      paid_ads         NaN                         1        79450.0   \n",
       "1  social_media      retail                         1        46992.0   \n",
       "2        events  healthcare                         5        78796.0   \n",
       "3      paid_ads      retail                         2        83843.0   \n",
       "4      referral   education                         3        85012.0   \n",
       "\n",
       "  employment_status       location  interaction_count  lead_score  converted  \n",
       "0        unemployed  south_america                  4        0.94          1  \n",
       "1          employed  south_america                  1        0.80          0  \n",
       "2        unemployed      australia                  3        0.69          1  \n",
       "3               NaN      australia                  1        0.87          0  \n",
       "4     self_employed         europe                  3        0.62          1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b4ecd63-c697-4e46-8cfc-0de2013a4a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retail\n"
     ]
    }
   ],
   "source": [
    "# Q1 What is the most frequent observation (mode) for the column industry?\n",
    "# Replace missing values\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].fillna('NA')\n",
    "    else:\n",
    "        df[col] = df[col].fillna(0.0)\n",
    "\n",
    "        # Find the most frequent observation (mode) for 'industry'\n",
    "mode_industry = df['industry'].mode()[0]\n",
    "print(mode_industry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ea5c00e-06d8-4b7f-8662-015d9ff6debc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix:\n",
      "                          number_of_courses_viewed  annual_income  \\\n",
      "number_of_courses_viewed                  1.000000       0.009770   \n",
      "annual_income                             0.009770       1.000000   \n",
      "interaction_count                        -0.023565       0.027036   \n",
      "lead_score                               -0.004879       0.015610   \n",
      "converted                                 0.435914       0.053131   \n",
      "\n",
      "                          interaction_count  lead_score  converted  \n",
      "number_of_courses_viewed          -0.023565   -0.004879   0.435914  \n",
      "annual_income                      0.027036    0.015610   0.053131  \n",
      "interaction_count                  1.000000    0.009888   0.374573  \n",
      "lead_score                         0.009888    1.000000   0.193673  \n",
      "converted                          0.374573    0.193673   1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Q2 Create the correlation matrix for the numerical features of your dataset. In a correlation matrix, you compute the correlation coefficient between every pair of features.\n",
    "# What are the two features that have the biggest correlation?\n",
    "\n",
    "# Compute correlation matrix for numerical features only\n",
    "corr_matrix = df.corr(numeric_only=True)\n",
    "\n",
    "# Display the matrix\n",
    "print(\"Correlation matrix:\")\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0620dfc-1ef6-4ee3-9c38-39c5af3d18bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Pairs Correlation:\n",
      "interaction_count and lead_score: 0.0099\n",
      "number_of_courses_viewed and lead_score: 0.0049\n",
      "number_of_courses_viewed and interaction_count: 0.0236\n",
      "annual_income and interaction_count: 0.0270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'annual_income and interaction_count'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate correlations for the specified pairs\n",
    "pairs = {\n",
    "    'interaction_count and lead_score': abs(corr_matrix.loc['interaction_count', 'lead_score']),\n",
    "    'number_of_courses_viewed and lead_score': abs(corr_matrix.loc['number_of_courses_viewed', 'lead_score']),\n",
    "    'number_of_courses_viewed and interaction_count': abs(corr_matrix.loc['number_of_courses_viewed', 'interaction_count']),\n",
    "    'annual_income and interaction_count': abs(corr_matrix.loc['annual_income', 'interaction_count'])\n",
    "}\n",
    "\n",
    "# Print all correlations\n",
    "print(\"\\nSelected Pairs Correlation:\")\n",
    "for pair, corr_value in pairs.items():\n",
    "    print(f\"{pair}: {corr_value:.4f}\")\n",
    "\n",
    "# Find the pair with the largest correlation\n",
    "max_pair = max(pairs, key=pairs.get)\n",
    "max_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f62e5eec-edca-4b9a-b947-85e89a14f47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(877, 292, 293)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate features and target\n",
    "df_full = df.copy()\n",
    "y = df_full['converted']\n",
    "X = df_full.drop(columns=['converted'])\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check sizes\n",
    "len(X_train), len(X_val), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0956327-70ae-4069-8960-a327635d2664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Feature  MI_Score\n",
      "0        lead_source      0.03\n",
      "1           industry      0.02\n",
      "2  employment_status      0.02\n",
      "3           location      0.00\n"
     ]
    }
   ],
   "source": [
    "# Q3 Calculate the mutual information score between converted and other categorical variables in the dataset. Use the training set only.\n",
    "# Round the scores to 2 decimals using round(score, 2).\n",
    "\n",
    "# Select categorical columns\n",
    "cat_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Encode categorical features into numeric codes (label encoding)\n",
    "X_train_enc = X_train[cat_cols].apply(lambda x: x.astype('category').cat.codes)\n",
    "\n",
    "# Compute mutual information between each categorical variable and target\n",
    "mi_scores = mutual_info_classif(X_train_enc, y_train, discrete_features=True, random_state=42)\n",
    "\n",
    "# Create DataFrame for better readability\n",
    "mi_df = pd.DataFrame({'Feature': cat_cols, 'MI_Score': mi_scores})\n",
    "mi_df['MI_Score'] = mi_df['MI_Score'].round(2)\n",
    "\n",
    "# Display results\n",
    "print(mi_df.sort_values(by='MI_Score', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c93f319-c8d8-4256-a818-02eedfdfac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 Now let's train a logistic regression.\n",
    "# Remember that we have several categorical variables in the dataset. Include them using one-hot encoding.\n",
    "# Fit the model on the training dataset.\n",
    "# To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:\n",
    "# model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "# Calculate the accuracy on the validation dataset and round it to 2 decimal digits.\n",
    "# What accuracy did you get?\n",
    "\n",
    "# Convert categorical + numerical data into dictionary format\n",
    "train_dicts = X_train.to_dict(orient='records')\n",
    "val_dicts = X_val.to_dict(orient='records')\n",
    "\n",
    "# One-hot encoding\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_train_enc = dv.fit_transform(train_dicts)\n",
    "X_val_enc = dv.transform(val_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24c9648d-2666-488a-a61e-f69c2c123037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n"
     ]
    }
   ],
   "source": [
    "# Define and fit model\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train_enc, y_train)\n",
    "\n",
    "# Predict on validation data\n",
    "y_pred = model.predict(X_val_enc)\n",
    "\n",
    "# Evaluate accuracy\n",
    "val_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(round(val_accuracy, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3fafdb8d-f735-43df-b3c5-33cac7d3f892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "industry: 0.0000\n",
      "employment_status: -0.0034\n",
      "lead_score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Q5 Let's find the least useful feature using the feature elimination technique.\n",
    "# Train a model using the same features and parameters as in Q4 (without rounding).\n",
    "# Now exclude each feature from this set and train a model without it. Record the accuracy for each model.\n",
    "# For each feature, calculate the difference between the original accuracy and the accuracy without the feature.\n",
    "\n",
    "features_to_check = ['industry', 'employment_status', 'lead_score']\n",
    "diffs = {}\n",
    "\n",
    "for feature in features_to_check:\n",
    "    # Drop one feature\n",
    "    X_train_drop = X_train.drop(columns=[feature])\n",
    "    X_val_drop = X_val.drop(columns=[feature])\n",
    "    \n",
    "    # Re-encode and train again\n",
    "    dv_drop = DictVectorizer(sparse=False)\n",
    "    X_train_enc_drop = dv_drop.fit_transform(X_train_drop.to_dict(orient='records'))\n",
    "    X_val_enc_drop = dv_drop.transform(X_val_drop.to_dict(orient='records'))\n",
    "    \n",
    "    model_drop = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "    model_drop.fit(X_train_enc_drop, y_train)\n",
    "    \n",
    "    # Compute accuracy without this feature\n",
    "    acc_drop = accuracy_score(y_val, model_drop.predict(X_val_enc_drop))\n",
    "    \n",
    "    # Store difference\n",
    "    diffs[feature] = base_acc - acc_drop\n",
    "\n",
    "# Display results\n",
    "for f, d in diffs.items():\n",
    "    print(f\"{f}: {d:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd7405bb-9c01-442d-a5b2-deac56697ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.01: 0.743, 0.1: 0.743, 1: 0.743, 10: 0.743, 100: 0.743}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q6 Now let's train a regularized logistic regression.\n",
    "# Let's try the following values of the parameter C: [0.01, 0.1, 1, 10, 100].\n",
    "# Train models using all the features as in Q4.\n",
    "# Calculate the accuracy on the validation dataset and round it to 3 decimal digits.\n",
    "# Which of these C leads to the best accuracy on the validation set?\n",
    "\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "acc_scores = {}\n",
    "\n",
    "for c in C_values:\n",
    "    model = LogisticRegression(solver='liblinear', C=c, max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_enc, y_train)\n",
    "    y_pred = model.predict(X_val_enc)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    acc_scores[c] = round(acc, 3)\n",
    "\n",
    "acc_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
